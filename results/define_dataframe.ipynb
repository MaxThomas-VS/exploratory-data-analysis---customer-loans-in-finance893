{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24db9e94",
   "metadata": {},
   "source": [
    "# Cleaning loan data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff9113",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import db_utils as dbu\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad0ca35",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Load raw data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25428f09",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/loan_payments-raw.csv')\n",
    "data_transform = dbu.DataTransform()\n",
    "data_frame_transform = dbu.DataFrameTransform()\n",
    "df_info = dbu.DataFrameInfo()\n",
    "plotter = dbu.Plotter()\n",
    "\n",
    "#%%[markdown]\n",
    "# Inspecting the dataframe we see over 50000 entries for 44 columns\n",
    "# Upon inspection, 'Unnamed: 0' duplicates the index, so we drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "print(df['Unnamed: 0'].head(5))\n",
    "df.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb25ca59",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "From prior knowledge of the dataset, we expect columns to be a mixture of categorical, numerical, and dates.\n",
    "We use the data_transform class to correct the dytpes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7103e7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "categorical_columns = ['grade', 'sub_grade', 'employment_length', 'home_ownership','verification_status','loan_status','payment_plan','purpose','term', 'collections_12_mths_ex_med','inq_last_6mths','delinq_2yrs','inq_last_6mths', 'policy_code','application_type']\n",
    "data_transform.MakeCategorical(df, categorical_columns)\n",
    "\n",
    "datenum_columns = ['issue_date', 'earliest_credit_line', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date']\n",
    "data_transform.Dates2Datetimes(df, datenum_columns)\n",
    "\n",
    "#%%[markdown]\n",
    "# We also see that sseveral columns have null values.\n",
    "# Four columns stand out with obviously high nan counts, and we drop these.\n",
    "# Three columns have less than 1% NaN, so we drop rows from these\n",
    "# We are left with columns that have too many missing values to drop either the rows or columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f868e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info.PrintNaNFractions(df)\n",
    "data_frame_transform.DropColsWithNaN(df, ['mths_since_last_delinq',\n",
    "                                          'mths_since_last_record',\n",
    "                                          'next_payment_date',\n",
    "                                          'mths_since_last_major_derog'])\n",
    "\n",
    "data_frame_transform.DropRowsWithNaN(df, ['last_payment_date',\n",
    "                                          'last_credit_pull_date',\n",
    "                                          'collections_12_mths_ex_med'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d78473e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ix = df[(df['home_ownership']=='NONE') | (df['home_ownership']=='OTHER')].index\n",
    "df.drop(ix, inplace=True)\n",
    "df['home_ownership'] = df['home_ownership'].cat.remove_unused_categories()    \n",
    "\n",
    "df_info.PrintNaNFractions(df)\n",
    "\n",
    "#%%[markdown] \n",
    "# We visualise these columns to impute these NaN thoughtfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e2d40c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plotter.InspectNaN(df, ['matrix','heatmap'])\n",
    "\n",
    "#%%[markdown]\n",
    "# The NaN look uncorrelated accross the data, and a heatmap of the correlation confirms this.\n",
    "# Next, we check the basic stats of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ddc7aa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df_info.PrintColumnInfo(df, 'funded_amount')\n",
    "df_info.PrintColumnInfo(df, 'term')\n",
    "df_info.PrintColumnInfo(df, 'int_rate')\n",
    "df_info.PrintColumnInfo(df, 'employment_length')\n",
    "\n",
    "#%%[markdown]\n",
    "# We check for correlations between the numerical variables, to see if we have options to impute by regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aaa3d2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "isnumeric = df_info.IsNumeric(df)\n",
    "plotter.CorrelationHeatmap(df, isnumeric)\n",
    "\n",
    "#%%[markdown]\n",
    "# Interest rate is poorly correlated with all variables (max or around 0.5).\n",
    "# We therefore impute this value with the median, which we see is similar to the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9916de8e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data_frame_transform.ImputeNaN(df, 'int_rate', 'median')\n",
    "\n",
    "#%%[markdown]\n",
    "# employment_length has 11 categories, none of which are dominant. \n",
    "# We therefore add an 'Unknown' category, rather than potentially biasing by imputing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a113860",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data_frame_transform.ImputeNaN(df, 'employment_length', 'Unknown')\n",
    "\n",
    "#%%[markdown]\n",
    "# funded_amount is strongly correlated with funded_amount_inv and installment.\n",
    "# We perform a multiple linear regression of \n",
    "# **funded_amount ~ 'funded_amount', 'funded_amount_inv', 'instalment','total_rec_int','total_rec_prncp','total_payment_inv','total_payment'**\n",
    "# to impute the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c49299e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "predictors = ['funded_amount_inv', 'instalment','total_rec_int','total_rec_prncp','total_payment_inv','total_payment']\n",
    "\n",
    "mlr_mask = data_frame_transform.DefineMLR2Impute(df, 'funded_amount', predictors)\n",
    "\n",
    "data_frame_transform.ImputeNaNMLR(df, 'funded_amount', predictors, mlr_mask)\n",
    "\n",
    "# %%[markdown]\n",
    "# term has two values, 36 months or 60 months. It is an important variable for later analysis.\n",
    "# They have similar frequencies so imputing with the most common is risky.\n",
    "# We therefore iteratively imputate term using a random forest method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa1c188",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data_frame_transform.ImputeTerm(df)\n",
    "df['term_numeric'] = [36 if ix == '36 months' else 60 for ix in df['term']]\n",
    "data_transform.MakeCategorical(df,['term'])\n",
    "#%%[markdown]\n",
    "# Now we've dealt with all the NaN, so we reindex to clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96bf58e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df_info.PrintNaNFractions(df)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "#%%[markdown]\n",
    "# Next we inspect the numerical data for skew. Where data are skewed away from normality, we test some transformations to reduce the skew.\n",
    "# skews with magnitudes less than 0.5 are acceptable, between 0.5 and 1 are moderate, and greater than 1 are severely skewed.\n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa70b49",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "skew = df.skew(numeric_only=True) \n",
    "very_skewed = list(skew.index[abs(skew)>=1])\n",
    "moderately_skewed = list(skew.index[(abs(skew)>=0.5) & (abs(skew)<1)])\n",
    "all_skewed = list(skew.index[abs(skew)>=0.5])\n",
    "\n",
    "print('Moderately skewed:')\n",
    "print(moderately_skewed)\n",
    "print('')\n",
    "print('Very skewed:')\n",
    "print(very_skewed)\n",
    "\n",
    "\n",
    "#%%[markdown]\n",
    "# We test log, box-cox, and yeo-johnson transformations on each skewed variable.\n",
    "# The yeo-johnson transofrmation performs best in all cases where a transofrmation is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3c8bb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "all_skewed.remove('member_id')\n",
    "all_skewed.remove('id')\n",
    "for col in all_skewed:\n",
    "    plotter.TransformTest(df, col=col)\n",
    "\n",
    "\n",
    "#%%[markdown]\n",
    "# Several variables look normal under the Yeo Johnson transformation:\n",
    "# *loan_amount, funded_amount, funded_amount_inv, instalment, annual_inc, open_accounts, total_accounts, total_payment, total_payment_inv, total_rec_prncp, total_rec_int*\n",
    "# We add columns for each of these with the YJ transformation applied.\n",
    "# The other variables are not amenable to transformation, and are often bimodal, so we leave these untouched. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3becf6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "to_transform = ['loan_amount', 'funded_amount', 'funded_amount_inv', 'instalment', 'annual_inc', 'open_accounts', 'total_accounts', 'total_payment', 'total_payment_inv', 'total_rec_prncp', 'total_rec_int']\n",
    "for col in to_transform:\n",
    "    new_col = col + '-yc'\n",
    "    df[new_col] = pd.Series(stats.yeojohnson(df[col])[0]).rename(new_col, inplace=True)\n",
    "\n",
    "df.info()\n",
    "\n",
    "#%%[markdown]\n",
    "# Next we visualise, identify, and remove outliers.\n",
    "# As a threshold, we remove outliers that are more extreme than 3 standard deviations from the mean.\n",
    "# We need an approximately normal distribution to apply the zscore criteria, so we use only columns which we performed the YC transformation on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21494318",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "to_check_outliers = [tt + '-yc' for tt in to_transform]\n",
    "rows_with_no_outliers = (np.abs(stats.zscore(df[to_check_outliers])) < 3).all(axis=1)\n",
    "df_test = df[rows_with_no_outliers]\n",
    "df_test.info()\n",
    "\n",
    "plotter.CheckOutlierRemoval(df, df_test, to_check_outliers)\n",
    "#%%[markdown]\n",
    "\n",
    "# We see that the oulier quality control step removed less than 2% of the data.\n",
    "# Histograms are similar before and after removal, and we see fewer outlier in the box and whisker plots.\n",
    "# We therefore go ahead and overwrite df with df_test, which has the outlier removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2cdcf4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "print('%s %% of data removed as outliers.' % (100*(len(df)-len(df_test))/len(df)))\n",
    "df = df_test\n",
    "\n",
    "#%%[markdown]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767d0b2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Next, we check for correlation between columns with approximately normal data.\n",
    "plotter.CorrelationHeatmap(df, to_check_outliers)\n",
    "\n",
    "#%%[markdown]\n",
    "# Several variable pairs have correlations > 0.9, with some rounding off to 1.\n",
    "# Rather than dropping variables now, we will use this figure as a reference during the analysis step.\n",
    "#\n",
    "# Finally, we save the dataframe to a csv for loading in the analysis step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddea98f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "data_transform.DropOnly1Value(df)\n",
    "print(df.info())\n",
    "df.to_csv('../data/loan_payments-clean.csv')\n",
    "df.to_pickle('../data/loan_payments-clean.pkl')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
